{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying removed users\n",
    "\n",
    "The following code accesses Twitter's API after the data collection period to see which accounts are not retrievable. Some in the literature use this as a form of establishjing ground turth for bot removal (assuming the accounts that were removed were removed because they are bot accounts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads tweets and generates a list of users\n",
    "#IMPORTS\n",
    "from os import listdir\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "#LOADING FROM RAW\n",
    "#Path to raw tweets\n",
    "path = \"\"\n",
    "users = []\n",
    "\n",
    "for month in ['08', '09', '10', '11', '12', '01', '02', '03', '04', '05', '06', '07']: #controls for month \n",
    "    data = {}\n",
    "    corrupted_counter = 0\n",
    "    for day in listdir(path):\n",
    "        if day.split('-')[1] == month:        \n",
    "            with open(path + '/' + day) as daily: \n",
    "                for line in daily:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                        id = tweet[\"id\"]\n",
    "                        data[id] = tweet\n",
    "                    except:\n",
    "                        with open(\"D:/Notebooks/Twitter_Preprocessed/Unreadable.txt\", 'w') as out_file:\n",
    "                            out_file.write(line)\n",
    "                        corrupted_counter += 1\n",
    "                        pass\n",
    "                    \n",
    "    print('for the month ' + month + ':')\n",
    "    print('loaded a total of ' + str(len(data.keys())) + ' tweets') #print total loads for month\n",
    "    print('there were ' + str(corrupted_counter) + ' unreadable tweets')\n",
    "\n",
    "    #Generating user list\n",
    "    counter = 0\n",
    "    for identifier in data.keys():\n",
    "        if data[identifier]['user']['id'] in users:\n",
    "            pass\n",
    "        else:\n",
    "            users.append(data[identifier]['user']['id'])\n",
    "            counter += 1\n",
    "    print('Added ' + str(counter) + ' unique users for the month')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This connects to the API and attempts to access the information for each account\n",
    "#IMPORTS\n",
    "import tweepy\n",
    "\n",
    "tweepy.debug(True)\n",
    "\n",
    "#AUTHENTICATION\n",
    "# Configured for your twitter account\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "#SETTING API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True) #handling of rate limit errors already automatic\n",
    "\n",
    "removed = []\n",
    "\n",
    "#checking if a user still exists in batches of a hundred users per call\n",
    "users = [str(user) for user in users]\n",
    "users_split = [users[i:i+100] for i in range(0, len(users), 100)]\n",
    "progress_counter = 0\n",
    "for user_group in users_split:\n",
    "    counter = 0\n",
    "    retrieved = api.lookup_users(user_ids=user_group)\n",
    "    retrieved = [user.id_str for user in retrieved]\n",
    "    for user in user_group:\n",
    "        if user in retrieved:\n",
    "            pass\n",
    "        else:\n",
    "            removed.append(user)\n",
    "            counter += 1\n",
    "    #print('Added ' + str(counter) + ' missing used ids')\n",
    "    \n",
    "    progress_counter += 1 \n",
    "    if progress_counter % 10 == 0:\n",
    "        print ('Processed '+str(progress_counter*100)+str('/')+str(len(users))+ ' users')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking through removed users' tweets\n",
    "\n",
    "The following code is basic exploration of the content of tweets from the users whose account was not retrievable after the data collection period. This essentially served (together with manual exploration of representative tweets and accounts) as a test of the hypothesis that these accounts are predominantly bots and that is why they were removed from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading only tweets from removed users from raw tweets\n",
    "#IMPORTS\n",
    "from os import listdir\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "#Path to raw tweets\n",
    "path = \"\"\n",
    "bot_data = defaultdict(dict)\n",
    "\n",
    "for month in ['08', '09', '10', '11', '12', '01', '02', '03', '04', '05', '06', '07']: #controls for month \n",
    "    counter = 0\n",
    "    for day in listdir(path):\n",
    "        if day.split('-')[1] == month:        \n",
    "            with open(path + '/' + day) as daily: \n",
    "                for line in daily:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                        if tweet['user']['id_str'] in removed_users:\n",
    "                            bot_data[tweet['user']['id_str']][tweet[\"id\"]] = tweet\n",
    "                            counter += 1\n",
    "                    except:\n",
    "                        pass\n",
    "    print('Finished for month ' + str(month))\n",
    "\n",
    "#This path is where the bot data json will be saved\n",
    "with open('path', 'w+') as outfile:\n",
    "    json.dump(bot_data, outfile)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This lemmatizes the tweets (this is necessary as not all these tweets made it through pre-processing)\n",
    "#This is once again running the Frog natural language processing suite, so it might require a virtual machines \n",
    "#IMPORTS\n",
    "import json\n",
    "import frog\n",
    "from collections import defaultdict\n",
    "\n",
    "#This path is where the bot data json is saved\n",
    "path = 'path'\n",
    "with open(path, 'r') as infile:\n",
    "    data = json.loads(infile.read())\n",
    "\n",
    "\n",
    "frog = frog.Frog(frog.FrogOptions(parser=True, ner=True))    \n",
    "counter = 0\n",
    "\n",
    "for user in data.keys():\n",
    "    for identifier in data[user].keys():\n",
    "        if data[user][identifier]['truncated'] is True:\n",
    "            tweet_raw = data[user][identifier]['extended_tweet']['full_text']\n",
    "        else:\n",
    "            tweet_raw = data[user][identifier]['text']\n",
    "\n",
    "        tweet_proc = frog.process(tweet_raw) \n",
    "\n",
    "        data[user][identifier]['lemmatized'] = [token['lemma'] for token in tweet_proc]\n",
    "        data[user][identifier]['tokenized'] = [token['text'] for token in tweet_proc]\n",
    "        data[user][identifier]['full_frog'] = tweet_proc\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "\n",
    "            print('processed ' + str(counter) + ' tweets')\n",
    "\n",
    "#This path is where the lemmatized bot data will be saved\n",
    "with open('path', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting user tweeting frequency\n",
    "#IMPORTS\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Potential loading of the data\n",
    "#with open('C:/Notebooks/Bots_lemmatized.json', 'r') as infile:\n",
    "#    bot_data = json.load(infile)\n",
    "\n",
    "volumes = {}\n",
    "for user in bot_data.keys():\n",
    "    volumes[user] = {'relevant': len(bot_data[user].keys())}\n",
    "    last_tweet = bot_data[user][list(bot_data[user].keys())[-1]]\n",
    "    start_time = datetime.strptime(last_tweet['user']['created_at'], '%a %b %d %X %z %Y')\n",
    "    end_time = datetime.strptime(last_tweet['created_at'], '%a %b %d %X %z %Y')\n",
    "    age = (end_time - start_time).total_seconds()/(24*60*60)\n",
    "    volumes[user]['general'] = last_tweet['user']['statuses_count']/age\n",
    "    \n",
    "to_plot = np.array([volumes[user]['general'] for user in bot_data.keys()])\n",
    "\n",
    "offset = 0\n",
    "bins = np.arange(0, 1000, step = 5)\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.xlim([min(to_plot)-offset, max(to_plot)+offset])\n",
    "plt.hist(to_plot, bins=bins, alpha=0.5, )\n",
    "plt.show()\n",
    "\n",
    "#It might also be useful to simply print out the values from largest to lowest to get a better idea of the upper extremes\n",
    "#This is not appropriately visible in the 'tail' of the graph\n",
    "print(sorted(to_plot, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic modelling the content of tweets from removed users\n",
    "#IMPORTS\n",
    "from gensim import corpora\n",
    "import json\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LdaModel\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "stops = set(stopwords.words('dutch'))\n",
    "\n",
    "#Get tweet list from overall data\n",
    "tweets = []\n",
    "for user in bot_data.keys():\n",
    "    for identifier in bot_data[user].keys():\n",
    "        if \"retweeted_status\" not in bot_data[user][identifier]:\n",
    "            tweet = []\n",
    "            for token in range(len(bot_data[user][identifier]['full_frog'])):\n",
    "                if bot_data[user][identifier]['full_frog'][token]['dep'] != 'punct':\n",
    "                    if bot_data[user][identifier]['full_frog'][token]['lemma'] not in stops:\n",
    "                        tweet.append(bot_data[user][identifier]['full_frog'][token]['lemma'])\n",
    "            tweets.append(tweet)\n",
    "\n",
    "#Get corpus from tweets\n",
    "dictionary = corpora.Dictionary(tweets)\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in tweets]\n",
    "\n",
    "#Training model and saving visualisation\n",
    "#Include the number of topics to model for in the list\n",
    "numbers = [20, 30, 40, 50]\n",
    "for number in numbers:\n",
    "    lda = LdaModel(corpus, num_topics=number, id2word=dictionary, alpha='auto', eta='auto',\n",
    "                   passes=6, iterations=100000000, gamma_threshold=0.001, chunksize=2000)\n",
    "    temp_file = \"bots_topics_\"+str(number)+\"_cleaned\"+\".html\"\n",
    "    vignette = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vignette, temp_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
