{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading tweets\n",
    "\n",
    "This script loads the stored tweets and pre-processes them. This includes the following steps:\n",
    "\n",
    "<ol>\n",
    "<li>Removing retweets (both formally identified and informally identified by beginning with \"RT\".</li>\n",
    "\n",
    "<li>Removing duplicate tweets.</li>\n",
    "\n",
    "<li>Removing tweets whose authors post too frequently (assumed bots), also stores the information on tweeting frequency as meta-data for individual tweets.</li>\n",
    "\n",
    "<li>Adding a processed version of quoted tweets by joining the text of the quoted tweet and the original text.</li>\n",
    "\n",
    "<li>Adding meta-data to tweets indicating what group of keywords is represented in the text of the tweet</li>\n",
    "</ol>\n",
    "\n",
    "The script can be made more efficient (it stores multiple copies of monthly data as it pre-processes). This feature was used in bug-fixing, but should be removed based on memory constrains (instead of generating new versions of the \"data\" dictionary each step can simpply re-write the record in the original \"data\" dictionary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from os import listdir\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "#LOADING FROM RAW\n",
    "\n",
    "path = \"C:/Notebooks/new\"\n",
    "removals = defaultdict(int)\n",
    "\n",
    "for month in ['08', '09']: #controls for month \n",
    "    data = {}\n",
    "    corrupted_counter = 0\n",
    "    for day in listdir(path):\n",
    "        if day.split('-')[1] == month:        \n",
    "            with open(path + '/' + day) as daily: \n",
    "                for line in daily:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                        id = tweet[\"id\"]\n",
    "                        data[id] = tweet\n",
    "                    except:\n",
    "                        with open(\"C:/Notebooks/Twitter_second/Unreadable.txt\", 'w+') as out_file:\n",
    "                            out_file.write(line)\n",
    "                        corrupted_counter += 1\n",
    "                        pass\n",
    "                    \n",
    "    print('for the month ' + month + ':')\n",
    "    print('loaded a total of ' + str(len(data.keys())) + ' tweets') #print total loads for month\n",
    "    print('there were ' + str(corrupted_counter) + ' unreadable tweets')\n",
    "    removals['total_tweets'] += len(data.keys())\n",
    "    removals['corrutped_tweets'] += corrupted_counter\n",
    "    \n",
    "#REMOVING RETWEETS\n",
    "#both formal and informal posts identified by 'RT' \n",
    "    \n",
    "    removed_tweets = defaultdict(dict) #To allow adding key:value pairs to non-existent keys\n",
    "\n",
    "    #formal retweets\n",
    "    data_v2 = {}\n",
    "    for identifier in data.keys():\n",
    "        if \"retweeted_status\" not in data[identifier]:\n",
    "            data_v2[identifier] = data[identifier]\n",
    "        else:\n",
    "            removed_tweets[identifier] = data[identifier]\n",
    "            removed_tweets[identifier]['reason'] = 'formal_retweet'\n",
    "    \n",
    "    #informal retweets    \n",
    "    data_v3 = {}\n",
    "    for identifier in data_v2.keys():\n",
    "        tweet = data_v2[identifier]['text']\n",
    "        if tweet.startswith(\"RT \") is True:\n",
    "            removed_tweets[identifier] = data[identifier]\n",
    "            removed_tweets[identifier]['reason'] = 'informal_retweet'\n",
    "        else: \n",
    "            data_v3[identifier] = data_v2[identifier]\n",
    "\n",
    "    print('removed ' + str(len(removed_tweets.keys())) + ' tweet as formal and informal re-tweets')  \n",
    "    removals['removed_retweets'] += len(removed_tweets.keys()) \n",
    "    \n",
    "#CHECK FOR AND REMOVAL OF DUPLICATES\n",
    "#Likely reduntant due to changes to ohow long tweets get collected\n",
    "\n",
    "    read = set()\n",
    "    data_v4 = {}\n",
    "    duplicates_counter = 0\n",
    "    for identifier in data_v3.keys():\n",
    "        if data[identifier]['truncated'] is True:\n",
    "            tweet = data[identifier]['extended_tweet']['full_text']\n",
    "        else:\n",
    "            tweet = data[identifier]['text']\n",
    "\n",
    "        if tweet in read:\n",
    "            removed_tweets[identifier] = data_v3[identifier]\n",
    "            removed_tweets[identifier]['reason'] = 'duplicate'\n",
    "            duplicates_counter += 1\n",
    "        else: \n",
    "            read.add(tweet)\n",
    "            data_v4[identifier] = data_v3[identifier]\n",
    "\n",
    "    print('removed ' + str(duplicates_counter) + ' tweets as duplicates')\n",
    "    removals['removed_duplicates'] += duplicates_counter\n",
    "    \n",
    "#REMOVING FREQUENT POSTERS (ASSUMED BOTS)\n",
    "#also including a 'monthly_relevant_tweets' and 'monthly_tweets' variable under 'user' for all tweets.\n",
    "\n",
    "    #Topic-relevant tweeting volume\n",
    "    users = defaultdict(int)\n",
    "    for identifier in data_v4.keys():\n",
    "        person = data_v4[identifier]['user']['screen_name']\n",
    "        users[person] += 1 \n",
    "    for identifier in data_v4.keys():\n",
    "        person = data_v4[identifier]['user']['screen_name']\n",
    "        data_v4[identifier]['user']['monthly_relevant_tweets'] = users[person]\n",
    "    \n",
    "    #General tweeting volume (using 30.44 as average days in a month)\n",
    "    for identifier in data_v4.keys():\n",
    "        start_time = datetime.strptime(data_v4[identifier]['user']['created_at'], '%a %b %d %X %z %Y')\n",
    "        end_time = datetime.strptime(data_v4[identifier]['created_at'], '%a %b %d %X %z %Y')\n",
    "        age = (end_time - start_time).total_seconds()/(24*60*60)\n",
    "        data_v4[identifier]['user']['monthly_tweets'] = data_v4[identifier]['user']['statuses_count']/(age/30.44) \n",
    "    \n",
    "    #actual filtering\n",
    "    data_v5 = {}\n",
    "    relevant_threshold = 450  #threshold with potentially important implications\n",
    "    general_threshold = 1500  #threshold with potentially important implications\n",
    "    relevant_counter = 0\n",
    "    general_counter = 0\n",
    "    for identifier in data_v4.keys():\n",
    "        if data_v4[identifier]['user']['monthly_relevant_tweets'] > relevant_threshold:\n",
    "            removed_tweets[identifier] = data[identifier]\n",
    "            removed_tweets[identifier]['reason'] = 'exceeded relevant tweet threshold of ' + str(relevant_threshold)\n",
    "            relevant_counter += 1\n",
    "        else:\n",
    "            if data_v4[identifier]['user']['monthly_tweets'] > general_threshold:\n",
    "                removed_tweets[identifier] = data[identifier]\n",
    "                removed_tweets[identifier]['reason'] = 'exceeded general tweet threshold of ' + str(general_threshold)\n",
    "                general_counter += 1\n",
    "            else:\n",
    "                data_v5[identifier] = data_v4[identifier]\n",
    "                \n",
    "    print('removed ' + str(relevant_counter) + ' tweets because user tweeted more than ' \n",
    "          + str(relevant_threshold) + ' relevant tweets this month')\n",
    "    print('removed ' + str(general_counter) + ' tweets because user tweets more than ' \n",
    "          + str(general_threshold) + ' tweets per month in the accounts lifetime')\n",
    "    removals['general_volume'] += general_counter\n",
    "    removals['relevant_volume'] += relevant_counter\n",
    "    \n",
    "#JOINING QUOTED TWEETS\n",
    "#adding a combination of original tweet and its quote separated by '||' as 'joined_text' to root of the tweet\n",
    "    \n",
    "    for identifier in data_v5.keys():\n",
    "        if data[identifier]['truncated'] is True:\n",
    "            tweet = data[identifier]['extended_tweet']['full_text']\n",
    "        else:\n",
    "            tweet = data[identifier]['text']\n",
    "\n",
    "        if \"quoted_status\" in data[identifier]:\n",
    "            data_v5[identifier]['joined_text'] = (data[identifier]['text'] +\n",
    "                                                   \"||\" +\n",
    "                                                   data[identifier]['quoted_status']['text'])\n",
    "            removals['quoted_tweets'] += 1\n",
    "        \n",
    "    print('Joined quoted tweets')\n",
    "\n",
    "#ADDING A LABEL BASED ON KEYWORD GROUP THAT MADE TWEET RELEVANT\n",
    "#This uses the same keywords as the listener to check which group of keywords made a given tweet relevant.\n",
    "\n",
    "    ecec = [\n",
    "        'kinderopvangtoeslag', 'kindgebonden budget', 'kinderbijslag',\n",
    "        'kinderopvang', 'kinder opvang', 'kinderdagverblijf',\n",
    "        'kdv', 'gastouder', 'gastouders',\n",
    "        'gastouderopvang', 'gastouder opvang', 'gastouderbureau',\n",
    "        'peuterspeelzalen', 'peuterspeelzaal', 'peuterspeelplaats',\n",
    "        'peutergroep', 'peutergroepen', 'buitenschoolseopvang',\n",
    "        'buitenschoolse opvang', 'naschoolseopvang', 'naschoolse opvang',\n",
    "        'naschoolse', 'BSO', 'voorschoolse opvang',\n",
    "        'voorschoolse', 'voorschoolseopvang', 'oppas',\n",
    "        'oppassers', 'babysitter', 'babysitters',\n",
    "        'nanny', 'nannies'\n",
    "        ]\n",
    "\n",
    "    lm_programmes = [\n",
    "        'Participatiewet', 'Participatie wet', 'Gesubsidieerde arbeid',\n",
    "        'opleiding', 'scholing', 'heropleiding',\n",
    "        'omscholing', 'training', 'retraining',\n",
    "        're-training', 'studie', 'studeren',\n",
    "        'praktijktraining', 'werkervaringsplek', 'stage',\n",
    "        'stage lopen', 'werkervaringsplek', 'werkervaring plek',\n",
    "        'studeer en werkplek', 'studeer- en werkplek', 'traineeship',\n",
    "        'Werkbedrijf', 'werk.nl', 'werkplein',\n",
    "        'werkpleinen', 'arbeidsadviseur', 'uwv',\n",
    "        'arbeidsbemiddelaar', 'arbeidsbemiddeling', 'loopbaan coach',\n",
    "        'werk coach', 'WW-uitkering', 'uitkering',\n",
    "        'bijstand', 'bijstandsuitkering', 'meewerkaftrek'\n",
    "        ]\n",
    "\n",
    "    lm_employment = [\n",
    "        'full-time werk', 'full time werk', 'fulltime werk',\n",
    "        'full-time baan'  'full time baan', 'fulltime baan',\n",
    "        'voltijd baan', 'voltijd werk', 'voltijdwerk',\n",
    "        '1 fte', '1 wtf', 'deeltijd werk',\n",
    "        'part-time werk', 'part time werk', 'deeltijd baan',\n",
    "        'part-time baan', 'part time baan', 'vast contract',\n",
    "        'vaste baan', 'vaste aanstelling', 'tijdelijk contract',\n",
    "        'tijdelijke baan', 'tijdelijke aanstelling', 'uitzendcontract',\n",
    "        'nul uren contract', '0 uren contract', 'zelfstandige zonder personeel',\n",
    "        'zzp', \"zzp'ers\", \"zzp'er\",\n",
    "        'zzper', 'zzpers', 'DBA modelovereenkomst',\n",
    "        'schijnzelfstandigheid', 'loondienst', 'in loondienst',\n",
    "        'eigen baas', 'eigen baas zijn'\n",
    "        ]\n",
    "\n",
    "    lm_phrases = [\n",
    "        'werkloosheid', 'werkeloosheid', 'werkloos',\n",
    "        'zonder baan', 'jobless'  'in between jobs',\n",
    "        'between jobs', 'in between two jobs', 'between two jobs',\n",
    "        'onderbezetting', 'onderbezet', 'zoek naar werk',\n",
    "        'kijken voor werk', 'een baan zoeken',\n",
    "        'zoeken naar een baan', 'banen zoeken', 'passend werk',\n",
    "        'passende arbeid', 'passende baan', 'passende job',\n",
    "        'goed werk', 'slecht werk', 'beter werk',\n",
    "        'betere kansen op werk', 'beter arbeidscontract', 'goed arbeidscontract',\n",
    "        'slecht arbeidscontract', 'vacature', 'vacatures',\n",
    "        'openstaande baan', 'vaardigheidseisen', 'ervaringseisen',\n",
    "        'werkervaring', 'werkervaringseisen', 'competenties'\n",
    "        ]\n",
    "    keywords = {'ecec': ecec, 'lm_programmes': lm_programmes, 'lm_employment': lm_employment, 'lm_phrases': lm_phrases}\n",
    "\n",
    "    for identifier in data_v5.keys():\n",
    "        if 'joined_text' in data_v5[identifier]: \n",
    "            tweet_raw = data_v5[identifier]['joined_text']\n",
    "        elif data_v5[identifier]['truncated'] is True:\n",
    "            tweet_raw = data_v5[identifier]['extended_tweet']['full_text']\n",
    "        else:\n",
    "            tweet_raw = data_v5[identifier]['text']\n",
    "        tweet = tweet_raw.lower()\n",
    "        \n",
    "        data_v5[identifier]['keyword_groups'] = []\n",
    "        data_v5[identifier]['keywords'] = []\n",
    "        for word_list in keywords.keys():\n",
    "            for keyword in keywords[word_list]: \n",
    "                if keyword in tweet:\n",
    "                    if keyword not in data_v5[identifier]['keywords']:\n",
    "                        data_v5[identifier]['keywords'].append(keyword)\n",
    "                    if word_list not in data_v5[identifier]['keyword_groups']:\n",
    "                        data_v5[identifier]['keyword_groups'].append(word_list)\n",
    "        \n",
    "    print('Added labels based on keywords')\n",
    "        \n",
    "#SAVE TWEETS\n",
    "\n",
    "    savename = str(month) + '_preprocessed'\n",
    "    with open('C:/Notebooks/Processed_2020/' + savename + '.json', 'w+') as outfile:\n",
    "        json.dump(data_v5, outfile)\n",
    "    print('Preprocessed tweets saved, total of ' + str(len(data_v5.keys())) + 'tweets')\n",
    "\n",
    "#(OPTIONAL) SAVE REMOVED TWEETS AS WELL\n",
    "\n",
    "#    savename2 = str(month) + '_removed'\n",
    "#    with open('D:/Notebooks/Twitter_Preprocessed/Removed/' + savename2 + '.json', 'w+') as outfile:\n",
    "#        json.dump(removed_tweets, outfile)\n",
    "#    print('Removed tweets saved')\n",
    "#    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
