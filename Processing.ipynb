{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Tweets\n",
    "\n",
    "Here the relevant tweets are tokenized and lemmatized. The suite used to do this is the Frog language processing suite published by Van den Bosch, A., Busser, G.J., Daelemans, W., and Canisius in 2007. The detailed documentation can be found at https://languagemachines.github.io/frog/\n",
    "\n",
    "The LaMachine distribution was used here, using a virtual machine and a vagrant client to access it (a Linux environment is required but the general analysis was ran in a Windows environment, hence the need for a virtual machine). This means that variable such as \"path\" here will depend on your system environment and installation of the Frog suite. \n",
    "\n",
    "This process is computationally expensive as it requires rather thorough analysis of the tweets. This means that processes such as morphological analysis or named-entity-recongnition are also ran to lemmatize the tweets. This script saves three processed versions of the tweets:\n",
    "<ol>\n",
    "<li> A simply tokenized version of the tweet </li>\n",
    "    \n",
    "<li> A lemmatized version of the tweet</li>\n",
    "    \n",
    "<li> A full token-by-token breakdown provided by frog (list of disctionaries each containign information for one token)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import frog\n",
    "from os import listdir\n",
    "\n",
    "#This path should correspond to where tweets are stored on your machine. \n",
    "#The monthly data files need to be named the same way the Preprocessing script saves them (\"##_string.json\")\n",
    "frog = frog.Frog(frog.FrogOptions(parser=True, ner=True))  \n",
    "\n",
    "path = '/vagrant/Processed_2020/'\n",
    "for month in ['08', '09']: #, '08', '09', '10', '11', '12', '01', '02']:  #controls for month \n",
    "    for file in listdir(path):\n",
    "        if file.split('_')[0] == month:   \n",
    "            with open(path + file, 'r') as infile:\n",
    "                data = json.loads(infile.read())\n",
    "            \n",
    "            print('Month ' + month + ':')\n",
    "            print('Loaded ' + str(len(data.keys())) + ' tweets')\n",
    "               \n",
    "            counter = 0\n",
    "                        \n",
    "            for identifier in data.keys():\n",
    "                if 'joined_text' in data[identifier]: \n",
    "                    tweet_raw = data[identifier]['joined_text']\n",
    "                elif data[identifier]['truncated'] is True:\n",
    "                    tweet_raw = data[identifier]['extended_tweet']['full_text']\n",
    "                else:\n",
    "                    tweet_raw = data[identifier]['text']\n",
    "\n",
    "                tweet_proc = frog.process(tweet_raw) \n",
    "                \n",
    "                #Here you can customize what information gets added to the tweets\n",
    "                data[identifier]['lemmatized'] = [token['lemma'] for token in tweet_proc]\n",
    "                data[identifier]['tokenized'] = [token['text'] for token in tweet_proc]\n",
    "                data[identifier]['full_frog'] = tweet_proc\n",
    "                \n",
    "                #This counter simply keeps track of the process\n",
    "                counter += 1\n",
    "                if counter % 2000 == 0:\n",
    "                    print('processed ' + str(counter) + ' tweets')\n",
    "            \n",
    "            #This determines the directory processed tweets are saved into\n",
    "            with open('/vagrant/' + month + '_processed.json', 'w') as outfile:\n",
    "                json.dump(data, outfile)\n",
    "                \n",
    "            print('Saved data for month ' + month)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative - stemming\n",
    "\n",
    "An alternative is to stem the tokens of a tweet rather than lemmatize them, which can be done using nltk. The results of this process were not as good as using lemmatization (by my estimation), but the process is much simpler and much faster to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.stem import SnowballStemmer\n",
    "from os import listdir\n",
    "\n",
    "#This path should correspond to where tweets are stored on your machine. \n",
    "#The monthly data files need to be named the same way the Preprocessing script saves them (\"##_string.json\")\n",
    "path = ''\n",
    "\n",
    "for file in listdir(path):\n",
    "    with open(path + file, 'r') as infile:\n",
    "        data = json.loads(infile.read())\n",
    "\n",
    "    print('Month ' + file.split('_')[0] + ':')\n",
    "    print('Loaded ' + str(len(data.keys())) + ' tweets')\n",
    "\n",
    "    counter = 0\n",
    "    stemmer = SnowballStemmer(\"dutch\")\n",
    "\n",
    "    for identifier in data.keys():\n",
    "        tweet_raw = data[identifier]['tokenized']\n",
    "        tweet_proc = [stemmer.stem(word) for word in tweet_raw]\n",
    "\n",
    "        data[identifier]['snowball'] = tweet_proc\n",
    "        \n",
    "        \n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "\n",
    "            print('processed ' + str(counter) + ' tweets')\n",
    "\n",
    "    with open(path + file.split('_')[0] + '_processed.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "    print('Saved data for month ' + file.split('_')[0])\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
