{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus sub-sections\n",
    "\n",
    "This script goes through two illustrative examples of zooming onto specific sub-sections of the corpus: The cost of early childhood education and care services and the sufficiency of unemployment insurance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introducing relevant words\n",
    "#These have been lemmatized usign the same language processing suite (Frog) so that they match the lemmatized tweets\n",
    "\n",
    "cost_words = ['goedkoop', 'duur', 'kostbaar', 'kost', 'prijs', 'betaalbare', 'onbetaalbaar']\n",
    "low_words = ['weinig', 'minder', 'laag', 'lager', 'genoeg', 'voldoende']\n",
    "unemployment_words = ['uwv', 'uitkering', 'bijstand', 'werkeloosheidsverzekering', 'werkeloosheid', 'ww']\n",
    "pronouns = ['ik', 'mij', 'mijn', 'me', 'we', 'wij', 'ons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING INDIVIDUAL TWEETS - ECEC COST\n",
    "from os import listdir\n",
    "import json\n",
    "import logging\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "path = 'PATH'\n",
    "tweets = {}\n",
    "stops = set(stopwords.words('dutch'))\n",
    "#keywords = ['ecec', 'lm_programmes', 'lm_employment', 'lm_phrases']\n",
    "\n",
    "\n",
    "for month in ['08', '09', '10', '11', '12', '01', '02', '03', '04', '05', '06', '07']: #controls for month \n",
    "    for file in listdir(path):\n",
    "        if file.split('_')[0] == month:   \n",
    "            with open(path + '/' + file, 'r') as infile:\n",
    "                data = json.loads(infile.read())\n",
    "                for identifier in data.keys(): \n",
    "                    #Only focusing on ECEC tweets\n",
    "                    if 'ecec' in data[identifier]['keyword_groups']:\n",
    "                        #Establishing if a tweet is concerned with cost\n",
    "                        cost_concerned = False\n",
    "                        for word in cost_words:\n",
    "                            if word in data[identifier]['lemmatized']:\n",
    "                                cost_concerned = True\n",
    "                                break\n",
    "                        #Establishing if a tweet is personal        \n",
    "                        personal = False \n",
    "                        for word in pronouns:\n",
    "                            if word in data[identifier]['lemmatized']:\n",
    "                                personal = True\n",
    "                                break\n",
    "                        \n",
    "                        #Actually saving the tweet \n",
    "                        if cost_concerned is True and personal is True:   \n",
    "                            tweet = []\n",
    "                            for token in range(len(data[identifier]['full_frog'])):\n",
    "                                #Removing punctuation\n",
    "                                if data[identifier]['full_frog'][token]['dep'] != 'punct':\n",
    "                                    #Removing stopwords\n",
    "                                    if data[identifier]['full_frog'][token]['lemma'] not in stops:\n",
    "                                        #Lowercasing all tokens\n",
    "                                        tweet.append(data[identifier]['full_frog'][token]['lemma'].lower())\n",
    "                            tweets[identifier] = tweet\n",
    "\n",
    "dictionary = corpora.Dictionary(tweets.values())\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in tweets.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING INDIVIDUAL TWEETS - UNEMPLOYMENT ADEQUACY\n",
    "from os import listdir\n",
    "import json\n",
    "import logging\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "path = 'PATH'\n",
    "tweets = {}\n",
    "stops = set(stopwords.words('dutch'))\n",
    "#keywords = ['ecec', 'lm_programmes', 'lm_employment', 'lm_phrases']\n",
    "\n",
    "\n",
    "for month in ['08', '09', '10', '11', '12', '01', '02', '03', '04', '05', '06', '07']: #controls for month \n",
    "    for file in listdir(path):\n",
    "        if file.split('_')[0] == month:   \n",
    "            with open(path + '/' + file, 'r') as infile:\n",
    "                data = json.loads(infile.read())\n",
    "                for identifier in data.keys(): \n",
    "                    #Establishing a tweet is relevant ot one of the labour market keywords\n",
    "                    if 'lm_programmes' or 'lm_employment' or 'lm_phrases' in data[identifier]['keyword_groups']:\n",
    "                        #Establishing if a tweet is concerned with unemployment insurance\n",
    "                        unemployment_concerned = False\n",
    "                        for word in unemployment_words:\n",
    "                            if word in data[identifier]['lemmatized']:\n",
    "                                unemployment_concerned = True\n",
    "                                break\n",
    "                        #Establishing if a tweet is concerned with sufficiency and insufficiency\n",
    "                        low_concerned = False\n",
    "                        for word in low_words:\n",
    "                            if word in data[identifier]['lemmatized']:\n",
    "                                low_concerned = True\n",
    "                                break\n",
    "                        #Establishing if a tweet is personal           \n",
    "                        personal = False \n",
    "                        for word in pronouns:\n",
    "                            if word in data[identifier]['lemmatized']:\n",
    "                                personal = True\n",
    "                                break\n",
    "                        \n",
    "                        #Actually saving the tweet\n",
    "                        if unemployment_concerned is True and low_concerned is True and personal is True:   \n",
    "                            tweet = []\n",
    "                            for token in range(len(data[identifier]['full_frog'])):\n",
    "                                #Remove punctuation\n",
    "                                if data[identifier]['full_frog'][token]['dep'] != 'punct':\n",
    "                                    #Remove stopwords\n",
    "                                    if data[identifier]['full_frog'][token]['lemma'] not in stops:\n",
    "                                        #Lowercase all tokens\n",
    "                                        tweet.append(data[identifier]['full_frog'][token]['lemma'].lower())\n",
    "                            tweets[identifier] = tweet\n",
    "\n",
    "dictionary = corpora.Dictionary(tweets.values())\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in tweets.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model\n",
    "\n",
    "The following generates LDA models of the loaded corpus sub-section and saves visualisations for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "import logging\n",
    "import json\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Include numbers of topics to train for in the \"numbers\" list \n",
    "numbers = []\n",
    "                     \n",
    "from gensim.models import LdaModel\n",
    "\n",
    "for number in numbers:\n",
    "    lda = LdaModel(corpus, num_topics=number, id2word=dictionary, alpha='auto', eta='auto', random_state=808,\n",
    "                   passes=50, iterations=100000000, gamma_threshold=0.001, chunksize=50)\n",
    "    \n",
    "    #Saving visualisation\n",
    "    temp_file = r\"lm_\" + str(number) + \".html\"\n",
    "    vignette = pyLDAvis.gensim.prepare(lda, corpus, dictionary, n_jobs=2, sort_topics=False, lambda_step=1.1) \n",
    "    pyLDAvis.save_html(vignette, temp_file)\n",
    "    \n",
    "    #Saving model itself\n",
    "    #temp_file = r\"ecec_\" + str(number)\n",
    "    #lda.save(temp_file)\n",
    "    \n",
    "    print('Finished for '+ str(number) +' alpha')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL STEP\n",
    "\n",
    "#This step is necessary ONLY if you remove the \"sort_topics=False\" attribute from generating the vignette.\n",
    "#That is because then the topic numbers between gensim model and pyLDAvis visualisation do nto correspond to one another\n",
    "\n",
    "#Load a specific model and print the relevant topics - necessary to establish which topics need to be focused on\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "#Insert path to model here\n",
    "model_file = r\"ecec_10\"\n",
    "lda = LdaModel.load(model_file)\n",
    "\n",
    "#Replace the number in range by the number of topics of a selected topic model\n",
    "for topic in range(10):\n",
    "    print('Topic ' +  str(topic))\n",
    "    print(lda.print_topic(topic, topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing most representative tweets of topics\n",
    "\n",
    "Before starting this process the selected model should be loaded as \"lda\" and you should manually a select a topic (or multiple topics) and not the numbers of those topics (in the gensim model).\n",
    "\n",
    "The result is a word document that lists the X tweets most representative of the topic Y from the loaded LDA model. The document includes the phi-score, user name, display name, and unprocessed text of the tweet (unprocessed for readability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "#Which topic is relevant\n",
    "selected_topic = 1\n",
    "#How many top tweets to save\n",
    "top_tweets = 50\n",
    "\n",
    "all_topics = lda.get_document_topics(corpus, per_word_topics=False)\n",
    "\n",
    "topic_info = defaultdict(dict)\n",
    "\n",
    "#Generates a dictionary where tweet ID is the key and value is another\n",
    "#dictionary where the key is \"topicx\" and value is the phis score for that topic.\n",
    "for identifier, topics in zip(tweets.keys(), all_topics):\n",
    "    for topic in topics:\n",
    "        if topic[0] == selected_topic:\n",
    "            topic_info[identifier]['topic'+str(selected_topic)] = topic[1]\n",
    "\n",
    "#This information can be included for any other topics of interest\n",
    "\n",
    "#Creates a selection of only tweets that are at least partially constituted from the topic of interest\n",
    "selection = {}\n",
    "for month in ['08', '09', '10', '11', '12', '01', '02', '03', '04', '05', '06', '07']: #controls for month \n",
    "    for file in listdir(path):\n",
    "        if file.split('_')[0] == month:   \n",
    "            with open(path + '/' + file, 'r') as infile:\n",
    "                data = json.loads(infile.read())\n",
    "                for identifier in data.keys():  \n",
    "                    if identifier in topic_info.keys():\n",
    "                        selection[identifier] = data[identifier]\n",
    "                        selection[identifier]['topic'+str(selected_topic)] = topic_info[identifier]['topic'+str(selected_topic)]\n",
    "                        \n",
    "sort = sorted(selection.values(), key=itemgetter('topic'+str(selected_topic)), reverse=True)\n",
    "\n",
    "#Change the name of the file tweets get saved into\n",
    "text_file = open(r\"name.doc\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for tweet in sort[:top_tweets]:\n",
    "    text_file.write('Phi score: ' + str(tweet['topic'+str(selected_topic)]))\n",
    "    text_file.write('Next User')\n",
    "    text_file.write('\\n')\n",
    "    if tweet['truncated'] is True:\n",
    "        text_file.write(tweet['extended_tweet']['full_text'])\n",
    "    else:\n",
    "        text_file.write(tweet['text'])\n",
    "    \n",
    "    text_file.write('\\n')\n",
    "    text_file.write('\\n')\n",
    "    \n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
