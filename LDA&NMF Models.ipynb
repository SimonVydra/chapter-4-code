{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA & NMF models\n",
    "\n",
    "This script is used to train and visualise LDA and NMF models There are three different ways to aggregate the existing tweets.\n",
    "<ol>\n",
    "<li>Individual documents correspond to individual tweets</li>\n",
    "    \n",
    "<li>Individual documents correspond to the aggregate of all tweets a single account authored</li>\n",
    "\n",
    "<li>Individual documents correspond to the aggregate of all tweets a single account authored in a given month</li>\n",
    "</ol>\n",
    "\n",
    "Through the itterative process the first (and simplest way) resulted in the most interpretable models.That is also the aggregation used to generate all models presented in the appendix.\n",
    "\n",
    "The first three blocks of code correspond to these three wyas of aggregating the tweets and are compatible with either the LDA model or the NMF model that follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads individual tweets and converts them into a corpus for gensim\n",
    "#IMPORTS\n",
    "from os import listdir\n",
    "import json\n",
    "import logging\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Path to processed tweets\n",
    "path = r'PATH'\n",
    "tweets = []\n",
    "stops = set(stopwords.words('dutch'))\n",
    "\n",
    "for month in ['01','02','03','04','05','06','07','08','09','10','11','12']: # controls for month \n",
    "    for file in listdir(path):\n",
    "        if file.split('_')[0] == month:   \n",
    "            with open(path + '/' + file, 'r') as infile:\n",
    "                data = json.loads(infile.read())\n",
    "                for identifier in data.keys():\n",
    "                    tweet = []\n",
    "                    for token in range(len(data[identifier]['full_frog'])):\n",
    "                        #Removing punctuation\n",
    "                        if data[identifier]['full_frog'][token]['dep'] != 'punct':\n",
    "                            #Removing stopwords\n",
    "                            if data[identifier]['full_frog'][token]['lemma'] not in stops:\n",
    "                                #Lowercasing all tokens\n",
    "                                tweet.append(data[identifier]['full_frog'][token]['lemma'].lower())\n",
    "                    tweets.append(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTERNATIVE TO THE SCRIPT ABOVE\n",
    "#This loads tweets aggregated to the level of authors (accounts), resulting in an author-LDA model. \n",
    "#IMPORTS\n",
    "from os import listdir\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from gensim import corpora\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Path to processed tweets\n",
    "path = 'PATH'\n",
    "author_tweets = defaultdict(dict)\n",
    "\n",
    "for month in ['01','02','03','04','05','06','07','08','09','10','11','12']: #controls for month \n",
    "    for file in listdir(path):\n",
    "        if file.split('_')[0] == month:\n",
    "            with open(path + '/' + file, 'r') as infile:\n",
    "                data = json.loads(infile.read())\n",
    "                for identifier in data.keys():\n",
    "                    user = data[identifier]['user']['id']\n",
    "                    tweet = []\n",
    "                    for token in range(len(data[identifier]['full_frog'])):\n",
    "                        #Removing punctuation\n",
    "                        if data[identifier]['full_frog'][token]['dep'] != 'punct':\n",
    "                            #Removing stopwords\n",
    "                            if data[identifier]['full_frog'][token]['lemma'] not in stops:\n",
    "                                #Lowercasing all tokens\n",
    "                                tweet.append(data[identifier]['full_frog'][token]['lemma'].lower())\n",
    "                    if user in author_tweets.keys():\n",
    "                        author_tweets[keyword][user]['text'] = author_tweets[user]['text'] + tweet\n",
    "                        author_tweets[keyword][user]['tweets'] += 1\n",
    "\n",
    "                    else:\n",
    "                        author_tweets[keyword][user] = {}\n",
    "                        author_tweets[keyword][user]['text'] = tweet\n",
    "                        author_tweets[keyword][user]['tweets'] = 1\n",
    "                        author_tweets[keyword][user]['user_info'] = data[identifier]['user']\n",
    "\n",
    "#Creating list of author-tweets\n",
    "tweets = [author_tweets[keyword][user]['text'] for user in author_tweets[keyword].keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTERNATIVE TO THE TWO SCRIPTS ABOVE\n",
    "#This loads tweets aggregated to the level of authors (accounts) AND to individual months.\n",
    "#IMPORTS\n",
    "from os import listdir\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from gensim import corpora\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Path to processed tweets\n",
    "path = 'PATH'\n",
    "author_tweets = {}\n",
    "for month in ['01','02','03','04','05','06','07','08','09','10','11','12']: #controls for month \n",
    "    for file in listdir(path):\n",
    "        if file.split('_')[0] == month:\n",
    "            monthly = defaultdict(list)\n",
    "            with open(path + '/' + file, 'r') as infile:\n",
    "                data = json.loads(infile.read())\n",
    "            for identifier in data.keys():\n",
    "                user = data[identifier]['user']['id']\n",
    "                tweet = []\n",
    "                for token in range(len(data[identifier]['full_frog'])):\n",
    "                    #Removing punctuation\n",
    "                    if data[identifier]['full_frog'][token]['dep'] != 'punct':\n",
    "                        #Removing stopwords\n",
    "                        if data[identifier]['full_frog'][token]['lemma'] not in stops:\n",
    "                            #Lowercasing all tokens\n",
    "                            tweet.append(data[identifier]['full_frog'][token]['lemma'].lower())\n",
    "                if user in author_tweets.keys():\n",
    "                    monthly[user] = monthly[user] + tweet\n",
    "                    author_tweets[user]['tweets'] += 1\n",
    "                    \n",
    "                else:\n",
    "                    author_tweets[user] = {}\n",
    "                    author_tweets[user]['text'] = []\n",
    "                    monthly[user] = monthly[user] + tweet\n",
    "                    author_tweets[user]['tweets'] = 1\n",
    "                    author_tweets[user]['user_info'] = data[identifier]['user']\n",
    "            \n",
    "            for user in monthly.keys():\n",
    "                author_tweets[user]['text'].append(monthly[user])\n",
    "                \n",
    "#Creating list of author-tweets\n",
    "tweets = []\n",
    "for user in author_tweets.keys():\n",
    "    tweets = tweets + author_tweets[user]['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This trains, saves, and visualises an LDA model\n",
    "#IMPORTS\n",
    "from gensim.models import LdaModel\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "import logging\n",
    "import json\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#generating corpus and dictionary for gensim\n",
    "dictionary = corpora.Dictionary(tweets, prune_at=None)\n",
    "corpus = [dictionary.doc2bow(tweet) for tweet in tweets]\n",
    "\n",
    "#Include topic numbers to model for in this list\n",
    "numbers = []\n",
    "                     \n",
    "#Models\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "for number in numbers:\n",
    "    #This trains the model itself.\n",
    "    lda = LdaModel(corpus, num_topics=number, id2word=dictionary, alpha='auto', eta='auto', random_state=808,\n",
    "                   passes=10, iterations=100000000, gamma_threshold=0.002, chunksize=50000)\n",
    "    \n",
    "    #Change the temp file here to where you want the visualisation stored\n",
    "    temp_file = r\"C:/LDA_\" + str(number) + \".html\"\n",
    "    vignette = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics = False)\n",
    "    pyLDAvis.save_html(vignette, temp_file)\n",
    "\n",
    "    \n",
    "    #Change the temp file here to where you want the model file stored\n",
    "    temp_file = r\"C:/Models/LDA_\" + str(number)\n",
    "    lda.save(temp_file)\n",
    "    \n",
    "    print('Finished for '+ str(number) +' topics')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This trains, saves, and visualises an NMF model\n",
    "#IMPORTS\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#generating vectorizer for SKlearn\n",
    "def nothing(x):\n",
    "    return x\n",
    "cv = CountVectorizer(tokenizer = nothing, preprocessor = nothing)\n",
    "fitted = cv.fit_transform(tweets)\n",
    "\n",
    "#Include topic numbers to model for in this list\n",
    "numbers = [35]\n",
    "                     \n",
    "#Models\n",
    "for number in numbers:\n",
    "    #This trains the NMF model\n",
    "    nmf = NMF(n_components=number, alpha=0.5, max_iter=100000, init='nndsvd', random_state=808)\n",
    "    W = nmf.fit_transform(fitted)\n",
    "\n",
    "    #Change the temp file here to where you want the model file stored    \n",
    "    temp_file = r\"C:/Notebooks/Appendix_Models/NMF_\"+str(number)+\"_label\"\n",
    "    pickle.dump(nmf, open(temp_file, 'wb'))\n",
    "    \n",
    "    #Change the temp file here to where you want the visualisation stored\n",
    "    temp_file = r\"C:/Notebooks/Appendix_Vignette/NMF_\"+str(number)+\"_label.html\"\n",
    "    vignette = pyLDAvis.sklearn.prepare(nmf, fitted, cv, lambda_step=1.1, sort_topics=False)\n",
    "    pyLDAvis.save_html(vignette, temp_file)\n",
    "    \n",
    "    print('Finished for '+ str(number) +' topics')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
